"convex set: semidefinite programming\nlanguage modeling: pretraining, masked language modeling, transformers\nk-means clustering: spectral clustering, expectation maximization, mixture model, model validation and evaluation\nsimilarity measures: gaussian kernel, spectral clustering, information retrieval, k-nearest neighbors\nneural network model: machine learning interpretability\ngeneralization: model selection\ncross-validation: hyperparameter estimation, k-fold cross-validation, model selection, model validation and evaluation, expected loss, generalization, hyperparameter optimization, generalization error\nvalidation: cross-validation\nneural networks: convolutional neural networks, hyperparameter optimization, backpropagation, universal approximation theorem, deep learning, recurrent neural networks, multi-class classification, hidden layer, regression, weight parameters, relu networks, gated recurrent unit, regularization, activation function, ocr, deep learning architectures, bias term, binary classification, overfitting, feedforward neural networks, adversarial machine learning, generalization, interpretability\ngraph theory: random graph, graph isomorphism, adjacency matrix, graph, weighted graph, max-cut problem\nunsupervised learning: anomaly detection, density estimation, clustering algorithms, representation learning, autoencoders, gaussian mixture model, generative modelling, isomap, clustering method, k-means clustering\ngraph representation: adjacency matrix, adjacency list\nmachine learning: classification, discrimination in machine learning, knowledge tracing, fairness, feature spaces, anomaly detection, kernel methods, convolutional neural networks, evaluation metrics in machine learning, postprocessing in machine learning, kernel trick, data generation, cross-validation, unsupervised learning, recommender system, model selection, model validation and evaluation, classification problems, representation learning, feature vector, supervised machine learning, word embedding, clustering, support vector machine, regression analysis, logistic regression, principal component analysis, data science, linear classifier, semi-supervised learning, kernel method, classifier, ethics in machine learning, deep learning, calibration in machine learning, time series analysis, random forest classifier, training set, preprocessing in machine learning, neural networks, ridge regression\nclassifier: model validation and evaluation, k-nearest neighbors, nearest neighbor classification, classification accuracy\nmodel comparison: model selection\nmean: variance, probability sampling, central limit theorem, standard deviation, skewness, variability, root mean square\nregression: prediction, ridge regression, lasso regression, nearest neighbor algorithms, linear regression, model validation and evaluation\nk-nearest neighbors: nearest neighbor classification\ndecision boundary: support vector machine\nkernel methods: principal component analysis, kernel trick, support vector machine\nlinear regression: polynomial regression, logistic regression, interpretability, least squares, neural networks, ridge regression, predictive modeling, curve fitting, linear model, regularization, generalized linear models, coefficient of determination, model validation and evaluation\nconvolutional layer: receptive field, translation equivariance, padding, zero padding\nmodel integration: model deployment\nmarkov models: stationary distribution\ngenerative modelling: normalizing flow, large language models, energy-based model, generative adversarial network, diffusion models\nmarkovâ€™s inequality: chebyshev's inequality, large deviations theory\ndeep learning: speech recognition, transfer learning, natural language processing, interpretability, binary classification, generative adversarial network, image generation, multi-class classification, object detection\nprobability: statistic, equally likely outcomes, mean, null hypothesis, statistical independence, a/b testing, sampling without replacement, conditional probability, joint distribution, machine learning, statistical inference, linear regression, hypothesis testing, variance, probability sampling, alternative hypothesis, statistical sampling, estimator, simulation, multiplication rule of probability, sampling variability, parameter, likelihood, randomization, standard deviation, stochastic processes, random walk, subjective probability, random sampling, covariance, probability distributions, population, standard error, sampling methods, random variable, correlation, sampling, confidence intervals, statistical association, resampling methods, bayesian knowledge tracing, test statistic, data science, estimation, sample, p-value, time series analysis, addition rule of probability\nstatistical bias: bias-variance tradeoff\ncoefficient of determination: model validation and evaluation\nstandard deviation: standard error, confidence intervals, chebyshev's inequality, normal distribution, regression, probability sampling, outliers\nrecursion: binary search, dynamic programming\nsensitive attributes: algorithmic fairness\nhinge loss: support vector machine\nmodel interpretation: machine learning interpretability\nrandom sampling: law of large numbers, inferential statistics, statistical test, sample mean\nobjective function: least squares, optimization problem\ncorrelation: linear regression, regression analysis, causality, data interpretation, correlation coefficient, regression\nhypothesis testing: randomized controlled trial, statistical significance, p-value\nexploratory data analysis: regression, linear regression, multivariate analysis, time series analysis\nconstrained optimization: linear programming, lagrange multipliers, penalty functions\nvariance: central limit theorem, confidence intervals, standard deviation, coefficient of determination\nsampling: empirical distribution function, hypothesis testing, probability sampling, random sampling, signal processing, confidence intervals, statistic\nlinear algebra: orthonormal basis, matrix rank, machine learning, coding theory, matrix inversion, error-correcting codes, projection matrix, lu decomposition\nconcentration inequalities: chernoff bound\ndata transformation: data standardization, feature extraction, model training, linear regression, data visualization\noverfitting: ridge regression, generalization, regularization, cross-validation\ndescriptive statistics: data cleaning, correlation, percentile\nbias-variance tradeoff: overfitting, cross-validation\ngraph: undirected graph, graph traversal, shortest path problem, max flow problem, directed graph, graph representation\nlu decomposition: matrix inversion\nfeature extraction: convolutional layer, logistic regression, naive bayes, feature selection, linear regression\nbayes theorem: posterior distribution, bayesian inference, maximum a posteriori estimation\nprogramming fundamentals: comparison operator, data structures, data types, function, model integration, expressions, sequence, control flow\ndata types: strings, data visualization, arrays, arithmetic operators, type conversion\nparameter initialization: relu networks\ntrain-test split: feature transformation, generalization, bootstrapping, model validation and evaluation\nleast squares: model fitting, curve fitting, linear regression, ridge regression\nconvexity: mean squared error\ndata import: data manipulation\nclassification methods: decision tree learning, k-nearest neighbors\nconfusion matrix: classification accuracy, algorithmic fairness, roc curve\ndecision tree: early stopping, random forests, ensemble methods, adaboost, bagging, boosting, decision tree learning, interpretability, pruning\nprobability sampling: empirical distribution function, data partitioning, statistical inference, central limit theorem, model validation and evaluation, stratified sampling, confidence intervals, resampling methods, sampling without replacement, test statistic, estimation\nboolean data type: control flow statements\ndistance metric: k-nearest neighbors, clustering, k-nearest neighbor graph\nrecommender system: implicit feedback, matrix factorization, collaborative filtering\nprobability distributions: variability, bernoulli distribution, discrete random variable, probability sampling, probability density function, statistic, mean, empirical distribution function, probability model, hypothesis testing, variance, parameter, sampling, central limit theorem, continuous random variable, random sampling, skewness\noptimization: stochastic gradient descent, greedy algorithms, gradient descent, knapsack problem, machine learning, dynamic programming\nbernoulli distribution: chernoff bound\nperformance metrics: model validation and evaluation, mean absolute error, mean squared error\nclassification: object detection, multi-class classification, model validation and evaluation, k-nearest neighbors, spam filtering, random forests, ensemble methods, image classification, knn, logistic regression, spam detection, linear classifier, binary classification, decision tree, classifier, performance metrics, ensemble learning, bayes classifier, confusion matrix, decision boundary, nearest neighbor algorithms, sentiment analysis\nstatistical methods: statistical modeling\ncomputational complexity: approximation algorithm, dynamic programming\ndata grouping: data aggregation\nlogistic function: logistic regression\nkernel method: kernel trick, support vector machine, principal component analysis\nfeature engineering: feature vector, spectral clustering, feature selection, feature importance\neigenvectors: graph laplacian, spectral clustering, random matrix theory\ndata visualization: error bars, scatter plot, histogram\nprogramming: python, object-oriented programming, data visualization, simulation\nbootstrapping: generalization, random forests, standard deviation, standard error\nregression analysis: prediction interval, model validation and evaluation, analysis of variance, model interpretation, regression model, mean squared error, linear regression, correlation coefficient\nfinite field: error-correcting codes\nridge regression: lasso regression, kernel trick, kernel methods\ncomputational thinking: data science\nsupervised machine learning: linear regression, neural networks, classification, regression, k-nearest neighbors, logistic regression, decision tree, classification methods\nassociation: correlation coefficient, correlation\nstatistic: empirical distribution function, statistical inference, hypothesis testing, model validation and evaluation, statistical bias, probability sampling\nsemidefinite programming: strong duality\nrecursive algorithms: dynamic programming, brute force algorithms\neigenvalue problem: eigenvectors\nsample statistic: statistical inference\nroc curve: area under the receiver operating characteristic curve\nmodel selection: information criteria, baseline models, cross-validation, train-test split, model complexity, parameter tuning, hyperparameter optimization, bootstrapping\nmodel optimization: model performance\npooling layer: average pooling, max pooling\nempirical distribution function: model validation and evaluation, p-value\nsimulation: empirical distribution function\ncosine similarity: contrastive learning\nweighted graph: shortest path problem, random walk\nprobability density function: fisher information, normal distribution\nsample mean: sampling distribution of the sample mean\nsample size: standard error, confidence intervals, central limit theorem, experimental design\napproximation: numerical integration\niterative optimization algorithms: stochastic gradient descent\nsum of independent random variables: concentration inequalities\nconvolutional neural networks: object detection, max pooling, residual networks, image classification\ndata frame: grouping operations, data preprocessing\nexperimental design: randomization, causality, treatment group, control group\nstrings: type conversion\nclustering: unsupervised learning, hierarchical clustering, k-means clustering, mixture model\nconfidence level: confidence intervals\ncoordinate descent: matrix factorization, k-means clustering\nmodel validation and evaluation: performance metrics, model comparison, confusion matrix, mean squared error, generalization, classification accuracy, bootstrapping, evaluation metrics, cross-validation, model selection, train-test split\ndata preprocessing: data encoding, linear regression, data transformation, data splitting, data analysis, data augmentation\nstochastic gradient descent: logistic regression\nloss functions: optimization problem, mean squared error, model selection, mean absolute error, supervised machine learning\nfisher information: cramer-rao lower bound\nfeature selection: random forest classifier\nquantitative data: hypothesis testing\nrecurrent neural networks: long short-term memory networks, gated recurrent unit, lstm\npython: associative array, function call, classification, data visualization, data processing, simulation, exception handling\neducational data mining: educational data science, educational analytics\ndata handling: data exploration, data aggregation, exploratory data analysis\ntransformer: language modeling, zero-shot learning, few-shot learning, sentiment classification\nnormal distribution: empirical rule, central limit theorem\ndata selection: data filtering, join operations, data grouping, data sorting\nregression model: ridge regression, linear regression, least absolute shrinkage and selection operator, logistic regression\nstatistics: simulation, inference, descriptive statistics, regression\nresampling methods: bootstrap method\ndata cleaning: educational data mining, data preprocessing\neigenvalue decomposition: eigenvectors, eigenvalues\ncentral limit theorem: normal approximation, confidence intervals\nsimulation modeling: monte carlo simulation\ngraph laplacian: spectral clustering, eigenvalue decomposition\nnull hypothesis: hypothesis testing, test statistic, p-value, alternative hypothesis\nlagrange multipliers: duality\nprincipal component analysis: reconstruction, eigenfaces, k-means clustering\nfairness: fairness metrics\nmachine learning interpretability: feature importance\nlinear model: polynomial basis, least squares, linear regression\nprobability theory: probability distributions, random variable, causality, concentration inequalities\nmatrix factorization: recommender system\nstatistical significance: hypothesis testing\nk-nearest neighbor graph: adjacency matrix\ndemographic disparities: algorithmic fairness\nconfidence intervals: hypothesis testing, error bars, resampling methods\nevaluation metrics: model validation and evaluation\nvisualization: data science\npositive semidefinite matrix: semidefinite programming\nensemble methods: random forests\ncurve fitting: model fitting, coefficient of determination\nmachine learning algorithms: feature extraction, ensemble methods\nconditional probability: dynamic bayesian network, bayes theorem\nbinary classification: sentiment analysis, confusion matrix, logistic regression\nstatistical estimation: confidence intervals\nprojection matrix: johnson-lindenstrauss lemma\np-value: statistical significance, significance level\noutliers: robust statistics\ngradient descent: logistic regression, hyperparameter optimization, momentum optimization, stochastic gradient descent\nrandom walk: stationary distribution, brownian motion\nprogramming languages: control flow statements, python, data types, data parallelism, model deployment\nloop constructs: breadth-first search\npreprocessing in machine learning: algorithmic fairness, machine learning pipeline\ndata augmentation: self-supervised learning\nlinear programming: duality\nsignal processing: signal reconstruction\ncomparison operator: predicate\nchain rule: backpropagation, gradient descent, non-convex optimization\ncommunity detection: spectral clustering\nregularization: model selection, weight decay, ridge regression, logistic regression, dropout\nbinary search: group testing\ncoding theory: linear code\nmaximum likelihood estimation: information criteria, standard error, logistic regression\nscatter plot: linear relationship\nneural network architecture: neural network model, gated recurrent unit, hyperparameter optimization, recurrent neural networks\ncategorical data: hypothesis testing, test statistic\nstatistical modeling: parameter estimation, hypothesis testing, simulation modeling, maximum likelihood estimation\nintegers: floating point numbers, arrays\ngrouping operations: aggregate functions\nsample: resampling methods\nspectral clustering: similarity matrix, model validation and evaluation\ndata standardization: z-score\nfeature spaces: kernel trick\nonline learning platforms: personalized learning\nbayesian inference: probabilistic graphical models\nstandard error: confidence intervals\nrandom variable: variance, independence\nlogistic regression: neural networks\nmissing data: data imputation\nalternative hypothesis: hypothesis testing, test statistic\nnondeterministic polynomial time hardness: hardness of approximation\nempirical risk minimization: support vector machine, gradient descent\neigenvalues: eigenvectors, power method\ngradient ascent: logistic regression\npython programming: data frame\nsupport vector machine: binary classification, kernel methods, support vectors\ninformation theory: deletion channel, erasure channel, conditional entropy, entropy\ncomputer science: data science\nstatistical distance: model validation and evaluation\nisomorphism: graph isomorphism\nquadratic programming: support vector machine\ninformation gain: decision tree\ngradient: hessian matrix, stochastic gradient descent\ndata sets: data handling\nevaluation metrics in machine learning: algorithmic fairness, model performance\ndata manipulation: data analysis\nlog likelihood: maximum likelihood estimation\noptimization problem: constrained optimization, dynamic programming, greedy algorithms\nlinear classifier: support vector machine, logistic regression, decision boundary\nself-supervised learning: masked language modeling, contrastive learning, language modeling\nmodel complexity: overfitting, generalization, bias-variance tradeoff\nsubjective probability: posterior probability\narrays: data science, lists, data table\nnatural language processing: machine translation, question answering\ntest statistic: hypothesis testing\nentropy: information gain\nclassification problems: expected risk, confusion matrix, loss functions\ndata imputation: expectation maximization\neuclidean distance: k-nearest neighbors\nalgorithm analysis: time complexity, space complexity, computational complexity\ncross entropy: language modeling\ndata table: relational join, data visualization\nexpected loss: standard error, generalization\ndata aggregation: time series analysis, data visualization\ncorrelation coefficient: mutual information, linear regression, regression\nmarkov process: bayesian knowledge tracing\nmean squared error: least squares\nfeature vector: clustering\ngaussian kernel: similarity matrix\ndimensionality reduction: principal component analysis, manifold learning, curse of dimensionality\ntext representation: nearest neighbor algorithms\ninferential statistics: confidence intervals, monte carlo simulation, probability sampling\nsearch tree: depth-first search\nprobabilistic model: maximum likelihood estimation, gaussian distribution\nundirected graph: spanning tree\nslope: linear regression\ncovariance matrix: gaussian distribution\nnewton's method: logistic regression\ninference: randomization, causality, data science\nbackpropagation: stochastic gradient descent\ndata analysis: feature engineering, classification, statistical modeling\nensemble learning: random forests\nmasked language modeling: sentiment analysis, named entity recognition\nsignal and noise: variance, bias\ndynamic bayesian network: intelligent tutoring systems\nconditional distribution: bayes classifier\nmajority voting: k-nearest neighbors\nconvex optimization: logistic regression, semidefinite programming\nexpressions: built-in functions\nposterior distribution: maximum a posteriori estimation\ngaussian mixture model: anomaly detection\nhilbert space: orthonormal basis\nhyperplane: support vector machine\neducational technology: online learning platforms\napproximation algorithm: randomized rounding, approximation ratio\ndensity estimation: autoregressive model, normalizing flow\ntime complexity: nondeterministic polynomial time hardness\nconcentration of measure: johnson-lindenstrauss lemma\ngraph traversal: depth-first search, breadth-first search\nnormalization: k-nearest neighbors\ngreedy algorithms: feature selection, knapsack problem\nestimation: inference, confidence intervals\nmodel fitting: predictive modeling, model validation and evaluation\nvariability: bias-variance tradeoff\ntraining set: train-test split, model selection, classifier\nclustering algorithms: spectral clustering, k-means clustering\nhierarchical clustering: dendrogram, agglomerative clustering\ndata processing: predictive modeling, data visualization\nrepresentation learning: pca, word embedding\nprobability space: random walk\ncomputer vision: object detection\nalgorithmic fairness: explainable artificial intelligence\ndistributed training: data parallelism\nactivation function: neural networks\nbootstrap resampling: confidence intervals\nrandom forests: model validation and evaluation\nmatrix rank: matrix inversion\ntest set: classification accuracy\nintercept: linear regression\nmax-cut problem: semidefinite relaxation\nencoder-decoder architecture: machine translation\nconvex duality: support vector machine\nmanifold learning: isomap\ndata science: predictive modeling\nclassification error: overfitting\nstatistical sampling: sample statistic\ndata partitioning: cross-validation\nfeature scaling: ridge regression\nobservational study: confounding variable\nprobability model: model validation and evaluation\nconditional entropy: information gain\nlink function: generalized linear models\nerror analysis: mean squared error\ndiscriminative model: generative adversarial network\npopulation: sample\noptimization techniques: dynamic programming\nnon-convex optimization: neural networks\nmathematical modeling: statistical modeling\npercentile: bootstrap method\nbagging: random forests\ntransfer learning: fine-tuning\nbayesian knowledge tracing: intelligent tutoring systems, personalized learning\ntokenization: transformer\ndatabase table: relational join\nerasure channel: deletion channel\npredictive modeling: classification, model validation and evaluation\ndirected graph: digraph\npretraining: zero-shot learning, few-shot learning\ndiscrimination in machine learning: algorithmic fairness\ngeneralized linear models: logistic regression, interpretability\ncontrol flow: loop\nparameter: statistical bias, statistic\ncausality: confounding variable\nmachine learning pipeline: training data in machine learning\nfunction: numerical integration\nconnected graph: spanning tree\nassignment statement: function call\nweak learner: adaboost\nconditional statements: loop constructs\ndata mining: educational data mining\npostprocessing in machine learning: algorithmic fairness\nethics in machine learning: algorithmic fairness\nstochastic processes: random walk\nrelational database: exploratory data analysis\nboosting: adaboost\nagglomerative clustering: distance metric\ncalibration in machine learning: algorithmic fairness\nprior information: posterior distribution\nstructural risk minimization: support vector machine\nparameter estimation: model parameters\ndata exploration: data analysis\nproportion: probability sampling\nadjacency matrix: graph laplacian\nclustering method: k-means clustering\nnumerical data: test statistic\njoint distribution: marginal distribution"