@inproceedings{warwick65543,
       publisher = {IATED Academy},
       booktitle = {6th International Conference on Education and New Learning Technologies},
         journal = {EDULEARN14 Proceedings },
            year = {2014},
           title = {Dropout rates of massive open online courses : behavioural patterns},
          editor = {L. G{\'o}mez Chova and A. L{\'o}pez Mart{\'i}nez and I. Candel Torres},
           pages = {5825--5834},
            issn = {2340-1117},
             url = {https://wrap.warwick.ac.uk/id/eprint/65543/},
        abstract = {Massive open online courses (MOOCs) have received wide publicity and many institutions have invested considerable effort in developing, promoting and delivering such courses. However, there are still many unresolved questions relating to MOOCs and their effectiveness. One of the major recurring issues raised in both academic literature and the popular press is the consistently high dropout rate of MOOC learners. Although many thousands of participants enrolled on these courses, the completion rate for most courses is below 13\%. This paper investigates MOOC attrition from several different perspectives. Firstly, we review existing literature relating to MOOC dropout rates, bringing together existing findings on completion rates and analyses of specific courses which identify factors that correlate to likelihood of dropout. We provide a meta-analysis of the basic figures on overall dropout rates previously collected to identify relationships between course factors and dropout rates. In addition, the literature is reviewed from a qualitative perspective drawing together perspectives on reasons for dropout and methods suggested for resolving or reducing the dropout rate. Secondly, using themes emerging from the initial investigation, we provide a preliminary analysis of data gathered from a Computing MOOC run by the University of Warwick, UK and presented using a Moodle platform. Different aspects of students' demographic data are examined to see if relationships to persistence exist. An important feature of this course is that it has been run in two different parallel modes ("traditional" MOOC mode with peer support, and "supported" mode with real time, tutored programming labs). This allows direct comparison between the dropout figures for the two different modes. Qualitative information from student evaluations is also considered. Finally, we discuss our findings relating MOOC dropout rates, considering what factors are within the control of a MOOC provider and suggesting the most promising avenues for improvement. Our results indicate that many participants who may be classed as dropouts (for example, because they do not complete the necessary components to gain a certificate) are still participating in the course in their own preferred way (either at a slower pace or with selective engagement). This suggests that the structure of "a course" may not be helpful to all participants and supporting different patterns of engagement and presentation of material may be beneficial.},
            isbn = {9788461705573},
          author = {Onah, Daniel F. O. and Sinclair, Jane and Boyatt, Russell}
}

@misc{wu2025federatedrlhfaggregatedclient,
      title={Towards Federated RLHF with Aggregated Client Preference for LLMs}, 
      author={Feijie Wu and Xiaoze Liu and Haoyu Wang and Xingchen Wang and Lu Su and Jing Gao},
      year={2025},
      eprint={2407.03038},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.03038}, 
}

@misc{wang2024large,
      title={Large Language Models for Education: A Survey and Outlook}, 
      author={Shen Wang and Tianlong Xu and Hang Li and Chaoli Zhang and Joleen Liang and Jiliang Tang and Philip S. Yu and Qingsong Wen},
      year={2024},
      eprint={2403.18105},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{ye2024openfedllmtraininglargelanguage,
      title={OpenFedLLM: Training Large Language Models on Decentralized Private Data via Federated Learning}, 
      author={Rui Ye and Wenhao Wang and Jingyi Chai and Dihan Li and Zexi Li and Yinda Xu and Yaxin Du and Yanfeng Wang and Siheng Chen},
      year={2024},
      eprint={2402.06954},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.06954}, 
}

@article{shih2008adaptive,
	abstract = {Adaptive e-learning is viewed as stimulation to support learning and improve student engagement, so designing appropriate adaptive e-learning environments contributes to personalizing instruction to reinforce learning outcomes. The purpose of this paper is to design an adaptive e-learning environment based on students' learning styles and study the impact of the adaptive e-learning environment on students'engagement. This research attempts as well to outline and compare the proposed adaptive e-learning environment with a conventional e-learning approach. The paper is based on mixed research methods that were used to study the impact as follows: Development method is used in designing the adaptive e-learning environment, a quasi-experimental research design for conducting the research experiment. The student engagement scale is used to measure the following affective and behavioral factors of engagement (skills, participation/interaction, performance, emotional). The results revealed that the experimental group is statistically significantly higher than those in the control group. These experimental results imply the potential of an adaptive e-learning environment to engage students towards learning. Several practical recommendations forward from this paper: how to design a base for adaptive e-learning based on the learning styles and their implementation; how to increase the impact of adaptive e-learning in education; how to raise cost efficiency of education. The proposed adaptive e-learning approach and the results can help e-learning institutes in designing and developing more customized and adaptive e-learning environments to reinforce student engagement.},
	author = {El-Sabagh, Hassan A. },
	date = {2021/10/01},
	date-added = {2025-06-04 09:10:53 +0200},
	date-modified = {2025-06-04 09:11:30 +0200},
	doi = {10.1186/s41239-021-00289-4},
	id = {El-Sabagh2021},
	isbn = {2365-9440},
	journal = {International Journal of Educational Technology in Higher Education},
	number = {1},
	pages = {53},
	title = {Adaptive e-learning environment based on learning styles and its impact on development students' engagement},
	url = {https://doi.org/10.1186/s41239-021-00289-4},
	volume = {18},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1186/s41239-021-00289-4}
}
	
@misc{hu2021loralowrankadaptationlarge,
      title={LoRA: Low-Rank Adaptation of Large Language Models}, 
      author={Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
      year={2021},
      eprint={2106.09685},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2106.09685}, 
}

@misc{wang2023aligning,
      title={Aligning Large Language Models with Human: A Survey}, 
      author={Yufei Wang and Wanjun Zhong and Liangyou Li and Fei Mi and Xingshan Zeng and Wenyong Huang and Lifeng Shang and Xin Jiang and Qun Liu},
      year={2023},
      eprint={2307.12966},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{schulman2017proximalpolicyoptimizationalgorithms,
      title={Proximal Policy Optimization Algorithms}, 
      author={John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
      year={2017},
      eprint={1707.06347},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1707.06347}, 
}

@article{Rafailov2023DirectPO,
      title={Direct Preference Optimization: Your Language Model is Secretly a Reward Model}, 
      author={Rafael Rafailov and Archit Sharma and Eric Mitchell and Stefano Ermon and Christopher D. Manning and Chelsea Finn},
      year={2023},
      volume={abs/2305.18290},
      journal={ArXiv},
      primaryClass={cs.LG}
}

@misc{casper2023open,
      title={Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback}, 
      author={Stephen Casper and al.},
      year={2023},
      eprint={2307.15217},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{mcmahan2023communicationefficientlearningdeepnetworks,
      title={Communication-Efficient Learning of Deep Networks from Decentralized Data}, 
      author={H. Brendan McMahan and Eider Moore and Daniel Ramage and Seth Hampson and Blaise Ag√ºera y Arcas},
      year={2023},
      eprint={1602.05629},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1602.05629}, 
}

@misc{stich2019localsgdconvergesfast,
      title={Local SGD Converges Fast and Communicates Little}, 
      author={Sebastian U. Stich},
      year={2019},
      eprint={1805.09767},
      archivePrefix={arXiv},
      primaryClass={math.OC},
      url={https://arxiv.org/abs/1805.09767}, 
}

@misc{li2023generativejudgeevaluatingalignment,
      title={Generative Judge for Evaluating Alignment}, 
      author={Junlong Li and Shichao Sun and Weizhe Yuan and Run-Ze Fan and Hai Zhao and Pengfei Liu},
      year={2023},
      eprint={2310.05470},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.05470}, 
}

@misc{dong2024surveyincontextlearning,
      title={A Survey on In-context Learning}, 
      author={Qingxiu Dong and Lei Li and Damai Dai and Ce Zheng and Jingyuan Ma and Rui Li and Heming Xia and Jingjing Xu and Zhiyong Wu and Tianyu Liu and Baobao Chang and Xu Sun and Lei Li and Zhifang Sui},
      year={2024},
      eprint={2301.00234},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2301.00234}, 
}

@misc{sahoo2025systematicsurveypromptengineering,
      title={A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications}, 
      author={Pranab Sahoo and Ayush Kumar Singh and Sriparna Saha and Vinija Jain and Samrat Mondal and Aman Chadha},
      year={2025},
      eprint={2402.07927},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2402.07927}, 
}

@inproceedings{vark-casestudy,
author = {Idrizi, Ermira and Filiposka, Sonja and Trajkovik, Vladimir},
year = {2018},
month = {09},
pages = {},
title = {VARK Learning Styles and Online Education: Case Study}
}

@article{Beurer_Kellner_2023,
   title={Prompting Is Programming: A Query Language for Large Language Models},
   volume={7},
   ISSN={2475-1421},
   url={http://dx.doi.org/10.1145/3591300},
   DOI={10.1145/3591300},
   number={PLDI},
   journal={Proceedings of the ACM on Programming Languages},
   publisher={Association for Computing Machinery (ACM)},
   author={Beurer-Kellner, Luca and Fischer, Marc and Vechev, Martin},
   year={2023},
   month=jun, pages={1946‚Äì1969} 
}

@misc{bonawitz2016practicalsecureaggregationfederated,
      title={Practical Secure Aggregation for Federated Learning on User-Held Data}, 
      author={Keith Bonawitz and Vladimir Ivanov and Ben Kreuter and Antonio Marcedone and H. Brendan McMahan and Sarvar Patel and Daniel Ramage and Aaron Segal and Karn Seth},
      year={2016},
      eprint={1611.04482},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/1611.04482}, 
}

@misc{lin2024limitedgeneralizationcapabilityimplicit,
      title={On the Limited Generalization Capability of the Implicit Reward Model Induced by Direct Preference Optimization}, 
      author={Yong Lin and Skyler Seto and Maartje ter Hoeve and Katherine Metcalf and Barry-John Theobald and Xuan Wang and Yizhe Zhang and Chen Huang and Tong Zhang},
      year={2024},
      eprint={2409.03650},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2409.03650}, 
}

@misc{ivison2024unpackingdpoppodisentangling,
      title={Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback}, 
      author={Hamish Ivison and Yizhong Wang and Jiacheng Liu and Zeqiu Wu and Valentina Pyatkin and Nathan Lambert and Noah A. Smith and Yejin Choi and Hannaneh Hajishirzi},
      year={2024},
      eprint={2406.09279},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.09279}, 
}

@misc{ouyang2022traininglanguagemodelsfollow,
      title={Training language models to follow instructions with human feedback}, 
      author={Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
      year={2022},
      eprint={2203.02155},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2203.02155}, 
}

@misc{lu2025machinelearningsyntheticdata,
      title={Machine Learning for Synthetic Data Generation: A Review}, 
      author={Yingzhou Lu and Lulu Chen and Yuanyuan Zhang and Minjie Shen and Huazheng Wang and Xiao Wang and Capucine van Rechem and Tianfan Fu and Wenqi Wei},
      year={2025},
      eprint={2302.04062},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2302.04062}, 
}

@Misc{accelerate,
  title =        {Accelerate: Training and inference at scale made simple, efficient and adaptable.},
  author =       {Sylvain Gugger and Lysandre Debut and Thomas Wolf and Philipp Schmid and Zachary Mueller and Sourab Mangrulkar and Marc Sun and Benjamin Bossan},
  howpublished = {\url{https://github.com/huggingface/accelerate}},
  year =         {2022}
}