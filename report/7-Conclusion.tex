This work represents a first step toward federated reinforcement learning from human feedback (RLHF) applied to education, with the goal of aligning large language models to learner preferences in a privacy-preserving way. Our main contributions include the deployment and adaptation of the \textsc{FedBiscuit} framework on EPFL's infrastructure, the design of a prompt-based synthetic data generation pipeline grounded in a structured learner model, and the integration of Weights \& Biases for training monitoring.

While we successfully trained the binary selectors and explored the impact of training data size, further improvements remain. Notably, aligning the model via DPO is still in progress, and training performance is limited by hardware constraints.

Next steps include running the full RLHF pipeline on real student data, improving scalability, and reducing training time. In future research, we also envision integrating real-time learner feedback, designing adaptive learning paths, and developing constrained generation techniques to further improve the quality and control of synthetic data.
